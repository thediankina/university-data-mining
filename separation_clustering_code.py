# -*- coding: utf-8 -*-
"""Галиулина_Разделительная_кластеризация.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15AC0EgxOUqrfWYt7UkiwIxUItb_qNdxQ
"""

!pip install scikit-learn-extra

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn_extra.cluster import KMedoids
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap
import seaborn as sns
import random

iris_dataset = pd.read_csv("iris.csv")
iris_dataset.info()

iris_dataset.head()

customers_dataset = pd.read_csv("customers.csv")
customers_dataset.info()

customers_dataset.head()

def clustering(dataset, n_clusters, algorithm):
  if algorithm == 'k-Means':
    model = KMeans(n_clusters=n_clusters).fit(dataset)
    centroids = model.cluster_centers_
    clusters = model.labels_
    SSE = model.inertia_
  elif algorithm == 'k-Medoids':
    model = KMedoids(n_clusters=n_clusters, init='k-medoids++').fit(dataset)
    centroids = model.cluster_centers_
    clusters = model.labels_
    SSE = model.inertia_
  new_dataset = dataset.copy()
  new_dataset['Clusters'] = clusters
  return centroids, clusters, SSE, new_dataset

def get_scale(dataset, df_columns):
  scaler = MinMaxScaler()
  scale = scaler.fit_transform(dataset[df_columns])
  df_scale = pd.DataFrame(scale, columns = df_columns)
  return df_scale

def get_results(dataset, algorithm, dimension, show_graph):
  dataset_scale = get_scale(dataset, dimension)
  dataset_labels = list(dataset_scale.columns.values)
  numbers_of_clusters = range(3, 10)
  centroids = []
  clusters = []
  SSEs = []
  datasets = []
  for number in numbers_of_clusters:
    cntrd, clstr, SSE, new_d = clustering(dataset_scale, number, algorithm)
    centroids.append(cntrd)
    clusters.append(clstr)
    SSEs.append(SSE)
    datasets.append(new_d)
  # print("Centoids:  ", centroids)
  # print("Clusters:  ", clusters)
  # print("SSEs:  ", SSEs)
  # print("datasets: ", datasets)
  if len(dimension) == 2 and show_graph == True:
    get_cluster_graph_2D(datasets, dataset_labels, algorithm)
  elif len(dimension) == 3 and show_graph == True:
    get_cluster_graph_3D(datasets, centroids, clusters, algorithm)
  return SSEs, numbers_of_clusters

def get_cluster_graph_2D(datasets, labels, algorithm):
  for i in range(0, 7):
    plt.figure(figsize=(9.01, 5))
    plt.title("Dataset: Iris\n" + "Algorithm: " + algorithm + "\nClusters: " + str(i+3))
    sns.scatterplot(x=labels[0], y=labels[1], hue='Clusters',  data=datasets[i], palette='bright')
    plt.grid(True)
    plt.show()

def get_cluster_graph_3D(datasets, centroids, labels, algorithm):
  data = []
  for d in datasets:
    data.append(d.values)
  for i in range(0,7):
    fig = plt.figure(figsize=(7.5, 5))
    ax = Axes3D(fig)
    ax.set_title("Dataset: Customers\n" + "Algorithm: " + algorithm + "\nClusters: " + str(i+3))
    label=datasets[i]['Clusters'].unique()
    cmap = ListedColormap(sns.color_palette("bright").as_hex())
    scatter = ax.scatter(data[i][:, 0], data[i][:, 1], data[i][:, 2], c=labels[i], label=label, cmap=cmap, alpha=0.25)
    ax.scatter(centroids[i][:, 0], centroids[i][:, 1], centroids[i][:, 2], marker='o', c='#000000', s=80, alpha=1)
    plt.legend(*scatter.legend_elements(), loc="upper left", title="Clusters")
    fig.show()

def get_SSE_graph(SSEs, ks, title=None):
  plt.figure(figsize=(9.08, 5))
  plt.title(label=title)
  plt.xlabel('k')
  plt.ylabel('Sum of Squared Errors (SSE)')
  plt.plot(ks[0],SSEs[0], label='Iris, K-Means', color='m')
  plt.plot(ks[1],SSEs[1], label='Iris, K-Medoids', color='b')
  plt.plot(ks[2],SSEs[2], label='Customers, K-Means', color='r')
  plt.plot(ks[3],SSEs[3], label='Customers, K-Medoids', color='g')
  plt.legend(loc='upper left')
  plt.grid(True)
  plt.show()

iris_dataset = pd.read_csv("iris.csv")
customers_dataset = pd.read_csv("customers.csv")

algorithms = ['k-Means', 'k-Medoids']
iris_dimension = ['sepal.length','petal.length']
customers_dimension = ['YearsEmployed', 'Income', 'CardDebt']
SSEs = []
ks = []
SSEs1, k1 = get_results(iris_dataset, algorithms[0], iris_dimension, True)
SSEs2, k2 = get_results(iris_dataset, algorithms[1], iris_dimension, True)
SSEs3, k3 = get_results(customers_dataset, algorithms[0], customers_dimension, True)
SSEs4, k4 = get_results(customers_dataset, algorithms[1], customers_dimension, True)
SSEs.append(SSEs1)
ks.append(k1)
SSEs.append(SSEs2)
ks.append(k2)
SSEs.append(SSEs3)
ks.append(k3)
SSEs.append(SSEs4)
ks.append(k4)
get_SSE_graph(SSEs, ks)

def make_noise(dataset, percent):
  dataset_labels = list(dataset.columns.values)
  random_label = random.randint(0, len(dataset_labels)-1)
  dataset_update=dataset.sample(frac=percent)
  dataset_update[random_label] = random.random()
  dataset.update(dataset_update)
  return dataset

percents = [0.01, 0.03, 0.05, 0.1]
titles = ['Noise: 1%', 'Noise: 3%', 'Noise: 5%', 'Noise: 10%']
i = 0
for percent in percents:
  noise_iris_dataset = make_noise(iris_dataset, percent)
  noise_customers_dataset = make_noise(customers_dataset, percent)
  SSEs = []
  ks = []
  SSEs1, k1 = get_results(iris_dataset, algorithms[0], iris_dimension, False)
  SSEs2, k2 = get_results(iris_dataset, algorithms[1], iris_dimension, False)
  SSEs3, k3 = get_results(customers_dataset, algorithms[0], customers_dimension, False)
  SSEs4, k4 = get_results(customers_dataset, algorithms[1], customers_dimension, False)
  SSEs.append(SSEs1)
  ks.append(k1)
  SSEs.append(SSEs2)
  ks.append(k2)
  SSEs.append(SSEs3)
  ks.append(k3)
  SSEs.append(SSEs4)
  ks.append(k4)
  get_SSE_graph(SSEs, ks, titles[i])
  i += 1